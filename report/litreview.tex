\section{Literature Review}
\label{literature_review}

In order to benchmark the computational and accuracy of Equilibrium Expectation as an ERGM estimation method we use the Stochastic Approximation and Markov Chain Maximum Likelihood Estimator as the basis. The aforementioned estimation approaches are detailed in this section, focusing largely on the philosophy of each algorithm reaching meaningful estimates.

We specifically note that though these techniques sought the ``actual'' MLE, it is worth beginning our discussion with the Maximum Pseudo Likelihood Estimation (MPLE) method (\cite{straussikeda1990}) as we use this method as one of the starting points in our benchmarking analysis.

\subsection{Maximum Pseudo-Likelihood Estimation and choice of starting point}

%strauss+ikeda1990

Due to the normalising constant referred to in Section \ref{introduction} being a summation over parameters of all possible graphs, estimating the true parameters of the ERGM is often an intractable problem. \citeauthor{straussikeda1990} proposed an approach where calculating the maximum likelihood function wasn't done on the direct likelihood but rather on a `pseudo' likelihood which approximated the true underlying model.

For a dyad independent network model, that is, a model where every dyad (or pair of ties) is independent from the next - maximum likelihood can be calculated. While this assumption simplifies the mathematics of estimation, it limits the practicality of the network model greatly. For dyad dependent networks, \citeauthor{straussikeda1990} proposed the pseudo-likelihood as the product of the probabilities of the $y_{ij}$, with each of the probabilities conditional on the rest of the data \citet{straussikeda1990}.

%ignoring $Z(\theta)$ through the use of conditioning on the rest of the data
When not conditioning on data, the ERGM takes the log linear form
\begin{equation}
\label{eqn:ergm_general_form}
    Pr(G) = {\frac{1}{Z(\theta)}}e^{\theta'x(G)}
\end{equation}
where $\theta$ is a vector of parameters and x(G) is a vector of graph statistics (on the observed graph).

Conditioning on the rest of the graph produces a model form that now does not depend on $\theta$:

\begin{equation}
\label{eqn:ergm_no_theta}
\begin{aligned}
P(y_{ij} = 1|C) = \frac{Pr(G^-)}{Pr(G^-) + Pr(G^+)} \\
logit(Pr(y_{ij} = 1|C) = \theta'{x(G^+) - x(G^-)} \\
logit(Pr(y_{ij} = 1|C) = \theta'\delta x_{ij}
\end{aligned}
\end{equation}

\subsection{Methods for Parameter Estimation}

\subsubsection{Monte Carlo Maximum Likelihood Estimation (MCMLE)}

The Monte Carlo Maximuim Likelihood Estimate was first introduced by \cite{geyerthompson1992}, and subsequently extended to curved ERGMs by \cite{hunterhandcock2006}. MCMLE is a method developed as a consequence to the fact that the  Laplace transformations (or normalising constants as is often used in this paper) for exponential family models with dependent data cannot be exactly calculated, and approximations are difficult to find. As such, the general idea comes down to translating from the intractable integral, to a probability distribution such that Monte Carlo methods become applicable, as shown with the method of moments equation below:

\begin{equation}
M_\theta(t) = \int{exp{\langle t(x), \tau \rangle}dP_\theta(x)} = \frac{c(\theta + t)}{c(\theta)}
\end{equation}

Where $t(x)$ are canonical statistics, and $P_\theta$ denoting the measure having density $f$ with respect to $\mu$.

The above refers to the ``Monte Carlo'' side of the eponymous section. Standard Gibbs or Metropolis sampling methods are used to generate an ergodic Markov chain $X_1$, $X_2$, ... having equilibrium distribution $P_\theta$. This leads to the observation that

\begin{equation}
\label{eq:mcmle_importance_sampling}
d_n(\theta') =  \frac{1}{n}\Sigma_{i=1}^n exp{\langle t(x_i), \theta' - \theta \rangle} \rightarrow d(\theta) = \frac{c(\theta')}{c(\theta)}
\end{equation}

almost surely by the ergodic theorem. For a given observation $x$ the log likelihood can be written as

\begin{equation}
I_x(\theta) = log{f_\theta(x)} + log{c(\theta)} = \langle t(x), \theta \rangle - log{d(\theta)}
\end{equation}

which has a Monte Carlo approximation given by 

\begin{equation}
\label{eq:log-likelihood-ratio}
I_{n,x}(\theta) = \langle t(x), \theta \rangle - log{d_n(\theta)}
\end{equation}

We see that Geyer and Thompson consider the log-likelihood ratio $I_{n,x}(\theta)$ at a fixed parameter choice rather than maximising the log-likelihood alone. Then, for any fixed $\theta$,

\begin{equation}
I_{n,x}(\theta) \rightarrow I_x(\theta)
\end{equation}

almost surely by expression \ref{eq:mcmle_importance_sampling}. Thus, under the concavity of $I_{n,x}(\theta)$ and $I_x(\theta)$ (a consequence of the exponential nature of the objective function in this case, though not a not necessary condition for the expression above to be true) we have that 

\begin{equation}
\hat{\theta}_n \rightarrow \hat{\theta} \text{almost surely}
\end{equation}

Using simulations from one distribution $P_\theta$ we are able to approximate $\hat{\theta}$ regardless of the intractability of the normalising constant. The algorithm has a large dependence on initialising with a reasonable starting point (i.e. something within the realm of reality), which sometimes means requiring a (sometimes infeasibly) large number of Monte Carlo samples. \cite{ergm} demonstrates the sensitivity to starting estimates by applying the MCMLE estimation method to the Sampson Monastery dataset, a dataset with 18 actors and 88 out of 306 possible ties. With a model based on edge count, and a starting point $\theta^0$ of 1, Hunter et al. shows that the model struggles to obtain the MLE in a single iteration, which wouldn't have been the case with a more reasonable starting estimate closer to the (already known) MLE. 

In addition, if the first derivative of the log-likelihood ratio in expression \ref{eq:log-likelihood-ratio} with respect to $\theta$ is negative (i.e. $g(Y) < g(Y_{obs})$) then \ref{eq:log-likelihood-ratio} will not have a maximum. Again, with a better starting estimate, this isn't likely to be the case but illustrates the sensitivity to challenging starting point estimates.

%%need to talk about hunter and handcock 2006
In his 2006 paper, Hancock applies the methods discussed in \cite{geyerthompson1992}:

The MLE $\boldsymbol{\hat{\theta}}$ satisfies the equation

\begin{equation}
\nabla l(\hat{\theta}) = \nabla \eta(\hat{\theta})^t [Z(y_{obs}) - E_{\eta(\hat{theta})}Z(Y) = 0
\end{equation}
%what is eta
Where $\nabla \eta(\hat{\theta})$ is a matrix of partial derivatives of $\eta$ with respect to $\theta$.

Noting that the exponential family form of the models makes the Fisher information matrix 

\begin{equation}
I(\theta) = \nabla \eta(\theta)^t[var_{\nabla \eta(\theta)}Z(Y)]\nabla \eta(\theta)
\end{equation}

It is easier to calculate than the Hessian matrix of second derivatives as in the Newton-Raphson method as used by Stochastic Approximation. The, we calculate iterations of the parameter $\theta$ as

\begin{equation}
\theta^{k+1} = \theta^{(k)} + [I(\theta^(k))]^{-1} \nabla l(\theta^{(k)})
\end{equation}

%%%%

The MCMC-MLE is theoretically expected to converge to the maximum likelihood assuming it exists.



\subsubsection{Robbins-Monro/ Stochastic Approximation}

Stochastic Approximation methods were among the first approaches to estimating the ERGM parameters by finding the actual MLE. First introduced by \citeauthor{robbinsmonro1951} in 1951, the stochastic approximation method aims to iteratively find the roots of an optimisation function as represented by an expected value. It does so by iteratively solving equations of the form 

\begin{equation}
E(Z_\theta) = 0
\end{equation}

Where $Z_\theta$ are the observed network statistics under the unknown $\theta$.

For a root proposed root $\hat{\theta}$ the \citeauthor{robbinsmonro1951} algorithms states that $\lim_{\hat{\theta_n} \rightarrow \inf} = \theta$, the true root. This is done by iterating through 

\begin{equation}
\label{eqn:rm_update}
\theta_{n+1} = \theta_{n} - \alpha_n(N(\theta) - \alpha)
\end{equation}



In the context of ERGMs, this is ...

A Markov chain is a sequence of random variables such that the value taken by the random variable only depends upon the value taken by the previous variable. We can hence consider a network in the form of an adjacency matrix in which each entry is a random variable. By switching the values of these variables to 0 or to 1(adding or removing a link from the network) one can generate a sequence of graphs such that each graph only depends upon the previous graph. This would be a Markov chain. The hypothesis is then that if the value at step t is drawn from the correct distribution than so will the value at step t+1. Unlike regular Monte-Carlo methods, the observations that are sampled are close to each-other since they vary by a single link. However, one would need a method for selecting which variable should change state in order to get closer to the MLE, this is done using the Metropolis-Hastings algorithm or the Gibbs sampler



%sneijders2002


Snijders introduced the Stochastic Approximation as an extension of the original Robbins-Monro algorithm in his paper \cite{snijders2001}. The algorithm in his subsequent paper (\cite{snijders2002}) takes the ideas proposed for this \textit{stochastic actor oriented model} and uses a simplification due to the scaling matrix used in the first phase (itself a matrix of derivatives) can be estimated using the covariance matrix of the generated statistics rather than by a finite quotient difference.
% ^rephrase

Due to the normalising constant in ERGMs $k\boldsymbol{\theta}$ typically being unknown, the means of estimating the probability distribution remains an intractable problem. And while stochastic approximation can be used to estimate the parameters, the rise of computing power has meant Markov Chain Monte Carlo MCMC simulation is typically used to address the value of $k\boldsymbol{\theta}$ (and the MLE as a whole).

Using MCMC estimation typically via the Metropolis-Hastings algorithm estimates network parameters by using the Markov process that asymptotically reaches a unique stationary distribution. A new state, $x'$ is proposed with some probability given by $q(x \rightarrow x'$. 


Snijder's algorithm consists of three main phases, with each phase adopting the Metropolis-Hastings sampling approach to draw from the model. Before diving into each of the phases, the developer has control on two parameters at this stage: the burn-in, and the gain factor. The burn in is a MCMC-specific hyperparameter and needs to be set for any MCMC-based method, while the gain factor controls the size of the steps the algorithm takes in reaching the MLE. A large gain factor means reaching the general vicinity of the MLE quicker, albeit with less `precise' movements. 

\paragraph{Phase 1: Initialisation}

The goal of Stochastic Approximation's first phase is to determine the scaling matrix $D_0$ and the initial values of the parameters being estimated using a small number of steps. The scaling matrix is used to scale the updates of the different elements of the parameter vector. At the end of the first phase, there is an optional initiation of the Newton-Raphson process (used in the subsequent step), $\hat{\theta}^{(N_1)} = \theta^{(1)} - \alpha_1 D^{-1}(\bar{u} - u_0)$ where the values $D$ and $\bar{u}$ are respectively defined as

\paragraph{Phase 2: Optimisation}

The second phase is the main phase of Stochastic Approximation, with a number of subphases each containing several iterations used to calculate $Y(n)$ according to the current parameter value $\hat{\theta}^{(n)}$. After of these minor iterations, the $\theta$ is updated according to the equation

\begin{equation}
\hat{\theta}^{(n+1)} = \hat{\theta}^{(n)} - \alpha_nD_0^{-1}Z(n)
\end{equation}

Where $\alpha_n$ is a constant consistent through each subphase, and $Z_k(n) = P(n)u(1-Y(n)) + (1 - P(n))u(Y(n)) - u_0$. At the end of each of these subphases, the algorithm estimates a new value for $\theta^{(n)}$, with the final estimate, $\hat{\theta}$, being the average of the $\theta^{(n)}$ estimated at the final subphase. 

Note that with each subphase, the value of $\alpha_n$ is halved, so that the steps become incrementally smaller with more iterations (i.e. the algorithm makes smaller, more 'careful' steps as it reaches the final $\hat{\theta}$).

\paragraph{Phase 3: Conversion check and measurement calculation}

As the previous phase calculated the final value for $\hat{\theta}$, the final phase of Stochastic Approximation seeks to estimate the covariance matrix of the estimator and $\Sigma(\theta)$. 


% why do we reach a unique stationary distribution?
\subsubsection{Equilibrium Expectation}

The focus of this paper, the equilibrium expectation algorithm by \cite{eqexpectation} proposes a fast algorithm for exponential random graph model parameters using maximum likelihood estimation which in turn affords an increase in the scale of the network being estimated. The fast and scalable nature of the equilibrium expectation algorithm are a consequence of the namesake: relying on properties of Markov chains at equilibrium.

As in the prior methods, estimation of the ERGM parameters is obtained from the method of moments equation

\begin{equation}
E_{\pi(\theta)}(z_A(x)) = z_A(x_{obs})
\end{equation}

where $z_A(x)$ are the network statistics and $x_{obs}$ is the observed network. 

As before, where the distribution of the normalising constant, $k(\theta)$, is not known, Markov chain Monte Carlo simulation is applied. Using MCMC simulation, we can compute the target probability given by expression \ref{eq:ergm_form} as well as the expected properties of the model, $E_{\pi(\theta)}(z_A(x)) = \Sigma_AZ_A(x)\pi(x, \theta)$. 

Using Metropolis-Hastings or other sampling methods (such as Gibbs) the algorithm asymptotically reaches a unique stationary distribution, $\pi(\theta)a$. After MCMC burnin, and the crux of the algorithm (deciding on a next step based on transition probabilities), the MCMC process eventually leads to equilibrium stationary distribution for all statistics $z_A(x)$

\begin{equation}
\label{eq:stationary_dist}
\Sigma_{x,x'}\pi(x, \theta)P(x \rightarrow x', \theta)(z_A(x') - z_A(x)) = 0
\end{equation}

While in other MCMC approaches, whenever a new $x_s(\theta)$ is drawn, the convergence criterion given in equation \ref{eq:stationary_dist} must be satisfied, in EE the convergence criterion is rewritten:

\begin{equation}
\label{eq:ee_stationary_dist}
E_{\pi(\theta)}(\Delta z_A(x, \theta)) = 0
\end{equation}

That is, if the network x is drawn from the stationary distribution $\pi(x,\theta)$ then the expected value in the change statistics $\Delta z_A(x,\theta)$ is zero. In doing so, the much more computationally expensive \ref{eq:stationary_dist} is reduced to a `cheaper' process, noting that \ref{eq:ee_stationary_dist} is only valid at equilibrium. 

On the assumption that a number of simulated networks exists, the left hand side of the stationary distribution \ref{ee_stationary_dist} can be calculated by Monte Carlo integration

%\begin{equation}
%\label{eq:ee_mc_int}
%\frac{1}{n} \Simga_i \Delta Z_A(x_{s_{i}}, \theta) = 0
%\end{equation}

Typically we estimate the value of $\theta^*$ by averaging the result of each estimated $\theta_{s_{i}}$ of an observed network $x_{s_{i}}$, a la $\hat{\theta*} = \frac{1}{n}\Sigma_i \theta(x_s)$. If the network $x_s$ is very large, then by the ergodicity of systems we can estimate $\theta*$ using $f_A(x_{obs}, \theta) = 0$ (\cite{stivala2018}), dropping the summation necessitated by MC integration. Thus, the true estimate for $\theta*$ can be found from the equilibrium expectation condition for all A,

\begin{equation}
\label{eq:ee}
\Delta z_A(x_s, \theta^{EE}) = 0 
\end{equation}

where the change statistic can be found by $\Delta z_a(x, \theta) = \Sigma_x' P(x \rightarrow x', \theta) (z_A(x') - z_A(x))$. \textit{If} the observed network $x_s$ is very large, then $\theta^{EE}(x_s)$ is the desired estimate for $\theta*$. Otherwise we require a large sample of networks to calculate the summation terms in the above expressions. 


Of course, in reality we only have one $x_{obs}$ drawn from some unknown probability distribution, $\pi*(x)$. Networks can be drawn from this unknown distribution by MCMC simulations, meaning we can then iteratively adjust $\theta$ until expression \ref{eq:ee} is satisfied. In contrast to methods described above (and others not described), the EE algorithm does not draw as many simulated networks for various $\theta$ values, making it noticeably quicker. A root finding method is still required once we have reached $\theta^{EE}$ that satisfies expression \ref{eq:ee} such as stochastic approximation methods (which we indeed use in the implementation of EE in \texttt{ergm} for the purpose of benchmarking).

% For a general MCMC algorithm (such as Metropolis-Hastings), once equilibrium is reached, we have that for all statistics $\sum_{x,x'}\pi(x, \boldsymbol{\theta})P(x \rightarrow x', \boldsymbol{\theta})(Z_A(x') - z_A(x)) = 0$. In contrast, EE replaces this algorithm which must converge for every simulated network $x_s(\boldsymbol{\theta})$ (making the entire estimation computationally quite expensive) with $E_{\pi(\boldsymbol{\theta})}(\DeltaZ_A(x, \boldsymbol{\theta}))$. This implies for a network drawn from a probability distribution $\pi(x, \boldsymbol{\theta})$


