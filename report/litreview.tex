\section{Literature Review}  

In order to benchmark the Equilibrium Expectation estimation method we start by considering the two classes from which currently popular estimation techniques lie: stochastic approximation and Monte Carlo Maximum Likelihood Estimation.

\subsection{Stochastic Approximation}

Stochastic Approximation methods were among the first approaches to estimating the ERGM parameters by finding the actual MLE. First introduced by \citeauthor{robbinsmonro1951} in 1951, the stochastic approximation method aims to iteratively find the roots of an optimisation function as represented by an expected value. For a root proposed root $\hat{\theta}$ the \citeauthor{robbinsmonro1951} algorithms states that $\lim_{\hat{\theta_n} \rightarrow \inf} = \theta$, the true root. This is done by iterating through 

\begin{equation}
\label{eqn:rm_update}
\theta_{n+1} = \theta_{n} - \alpha_n(N(\theta) - \alpha)
\end{equation}

Where 


In the context of ERGMs, this is ...

Implemented in 2002 by Snijders...


We specifically note that the these techniques sought the ``actual'' MLE, as it is worth beginning our discussion with the Maximum Pseudo Likelihood Estimation of ERGM parameters \citet{straussikeda1990}.

%strauss+ikeda1990

Due to the normalising constant, generating the parameters of the ERGM is usually intractable. \citeauthor{straussikeda1990} proposed an approach where calculating the maximum likelihood function wasn't done on the direct likelihood, rather on a 'pseudo' likelihood.

For a dyad independent network model, that is, a model where every dyad (or pair of ties) is independent from the next - maximum likelihood can be calculated. While this assumption simplifies the mathematics of estimation, it limits the practicality of the network model greatly. For dyad dependent networks, \citeauthor{straussikeda1990} proposed the pseudo-likelihood as the product of the probabilities of the $y_{ij}$, with each of the probabilities conditional on the rest of the data \citet{straussikeda1990}.

%ignoring $Z(\theta)$ through the use of conditioning on the rest of the data
When not conditioning on data, the ERGM takes the log linear form
\begin{equation}
\label{eqn:ergm_general_form}
    Pr(G) = {\frac{1}{Z(\theta)}}e^{\theta'x(G)}
\end{equation}
where $\theta$ is a vector of parameters and x(G) is a vector of graph statistics (on the observed graph).

Conditioning on the rest of the graph produces a model form that now does not depend on $\theta$
\begin{equation}
\begin{aligned}
    Pr(y_{ij} = 1|C) = \frac{Pr(G^-)}{Pr(G^-) + Pr(G^+)} \\
    logit(Pr(y_{ij} = 1|C) = \theta'{x(G^+) - x(G^-)} \\
    \label{eqn:ergm_no_theta}
    logit(Pr(y_{ij} = 1|C) = \theta'\delta x_{ij}
\end{aligned}
\end{equation}
%sneijders2002
\subsection{Monte Carlo Maximum Likelihood Estimation (MCMLE)}

Due to the normalising constant in ERGMs $k\boldsymbol{\theta}$ typically being unknown, the means of estimating the probability distribution remains an intractable problem. And while stochastic approximation can be used to estimate the parameters, the rise of computing power has meant Markov Chain Monte Carlo MCMC simulation is typically used to address the value of $k\boldsymbol{\theta}$ (and the MLE as a whole).

Using MCMC estimation typically via the Metropolis-Hastings algorithm estimates network parameters by using the Markov process that asymptotically reaches a unique stationary distribution. A new state, $x'$ is proposed with some probability given by $q(x \rightarrow x'$. 

% why do we reach a unique stationary distribution?
\subsection{Equilibrium Expectation}

The focus of this paper, the equilibrium expectation algorithm by \citet{eqexpectation} proposes a fast algorithm for exponential random graph model parameters using maximum likelihood estimation which in turn affords an increase in the scale of the network being estimated. The "fast" and "scalable" nature of the equilibrium expectation algorithm are a consequence of the 

For a general MCMC algorithm (such as Metropolis-Hastings), once equilibrium is reached, we have that for all statistics $\sum_{x,x'}\pi(x, \boldsymbol{\theta})P(x \rightarrow x', \boldsymbol{\theta})(Z_A(x') - z_A(x)) = 0$. In contrast, EE replaces this algorithm which must converge for every simulated network $x_s(\boldsymbol{\theta})$ (making the entire estimation computationally quite expensive) with $E_{\pi(\boldsymbol{\theta})}(\DeltaZ_A(x, \boldsymbol{\theta}))$. This implies for a network drawn from a probability distribution $\pi(x, \boldsymbol{\theta})$

\subsection{Techniques for handling Missing Data}